{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchkeras.LightModel can't be used!\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import torch \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torchkeras\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module): \n",
    "    def __init__(self, input_size, output_size, layer_num, hidden_size, activation) -> None:\n",
    "        super().__init__() \n",
    "        if activation == 'R': self.activation = nn.ReLU() \n",
    "        if activation == 'T': self.activation = nn.Tanh() \n",
    "        if activation == 'S': self.activation = nn.Sigmoid() \n",
    "        \n",
    "        self.layer_num = layer_num\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        def normal(shape):\n",
    "            import math\n",
    "            return (torch.rand(shape) - 0.5) * 1.0 / math.sqrt(150)\n",
    "        self.W_hx = nn.Parameter(normal((self.layer_num, self.input_size, self.hidden_size)))  \n",
    "        self.b_hx = nn.Parameter(normal((self.layer_num, self.hidden_size)))\n",
    "        self.W_hh = nn.Parameter(normal((self.layer_num, self.hidden_size, self.hidden_size))) \n",
    "        self.b_hh = nn.Parameter(normal((self.layer_num, self.hidden_size)))\n",
    "        # self.W_ho = {i : nn.Linear(self.hidden_size, self.hidden_size) for i in range(0, self.layer_num)}\n",
    "        self.O = nn.Linear(hidden_size, output_size) \n",
    "        \n",
    "        # for x in self.W_hx.values(): \n",
    "        # self.reset_parameters(self.W_hx) \n",
    "        # for x in self.W_hh.values(): \n",
    "            # self.reset_parameters(x) \n",
    "        # for x in self.W_ho.values(): \n",
    "            # self.reset_parameters(x) \n",
    "        # self.reset_parameters(self.O) \n",
    "\n",
    "    \n",
    "    def reset_parameters(self, x):\n",
    "        # print(x)\n",
    "        import math\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in x.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "            # weight = nn.Parameter(weight)\n",
    "            # print(weight.requires_grad)\n",
    "            # print(weight)\n",
    "\n",
    "\n",
    "    def forward(self, X): \n",
    "        X = {i : X[:, :, i] for i in range(self.layer_num)}\n",
    "        H = {}; O = {}\n",
    "        H[0] = torch.zeros(self.hidden_size)#, requires_grad=True) # \n",
    "\n",
    "        for i in range(0, self.layer_num): \n",
    "            # if i == 14:\n",
    "                # print(self.W_hh[i](H[i]).shape, self.W_hx[i](X[i])[13], self.W_hx[i](X[i])[17])\n",
    "            H[i + 1] = self.activation(H[i] @ self.W_hh[i] + self.b_hh[i] + X[i] @ self.W_hx[i] + self.b_hx[i]) \n",
    "            # O[i] = self.activation(self.W_ho[i](H[i]))\n",
    "        # print(H[3][1], H[3][2],H[3].shape)\n",
    "        # print(self.O(O[self.layer_num - 1])[:5]) \n",
    "        return self.O(H[self.layer_num]).squeeze() \n",
    "\n",
    "R = RNN(28, 10, 28, 128, 'R') \n",
    "x = torch.rand(64, 28, 28) \n",
    "R(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "batch_size, num_steps = 32, 35\n",
    "# train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "\n",
    "\n",
    "# F.one_hot(torch.tensor([0, 2]), len(vocab))\n",
    "X = torch.arange(10).reshape((2, 5))\n",
    "F.one_hot(X.T, 28).shape\n",
    "\n",
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device),)\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    print(\"wa\")\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    print(inputs.shape, \"haha\")\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)\n",
    "\n",
    "class RNNModelScratch(nn.Module):  \n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device, get_params,\n",
    "                 init_state, forward_fn):\n",
    "        super().__init__()\n",
    "        print(\"haha\")\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        print(\"wqe\")\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)\n",
    "\n",
    "# num_hiddens = 512\n",
    "# net = RNNModelScratch(28, 150, d2l.try_gpu(), get_params,\n",
    "#                       init_rnn_state, rnn)\n",
    "# state = net.begin_state(X.shape[0], d2l.try_gpu())\n",
    "# Y, new_state = net(X.to(d2l.try_gpu()), state)\n",
    "# Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_RNN(nn.Module): \n",
    "    def __init__(self, input_size, output_size, layer_num, hidden_size, activation):\n",
    "        super(nn_RNN,self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = x.squeeze().permute(2, 0, 1)\n",
    "        r_out,h_n = self.rnn(x,None)\n",
    "        # print(r_out.shape)\n",
    "\n",
    "        out = self.out(r_out[-1 ,:,:])\n",
    "\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "\n",
    "        # b * 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class o_RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=28,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Linear(64,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28, 28)\n",
    "        r_out,h_n = self.rnn(x,None)\n",
    "        out = self.out(h_n[-1])\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dee909b94f9b71f1735c26369abf02849623765c9fac88e4d4cba156fc12504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
