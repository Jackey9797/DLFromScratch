{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import torch \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torchkeras\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, input_size, input_channel, activation='S', alpha=4) -> None:\n",
    "        super().__init__() \n",
    "        if activation == 'S': self.activation = nn.Sigmoid()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(*[\n",
    "            nn.Linear(input_channel, input_channel // alpha), \n",
    "            self.activation, \n",
    "            nn.Linear(input_channel // alpha, input_channel),\n",
    "            self.activation, \n",
    "        ])\n",
    "\n",
    "    def forward(self, x): \n",
    "        # print(x.shape, self.net(x).shape)\n",
    "        y = self.pool(x).view(x.shape[0],-1) \n",
    "        # print(y.shape)\n",
    "        y = self.fc(y).view(y.shape[0],x.shape[1], 1, 1) \n",
    "        return x * y.expand_as(x)   \n",
    "\n",
    "# C = ChannelAttention(64, 16)\n",
    "# x = torch.rand((3, 16, 8, 8))\n",
    "# C(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):                    #继承来着nn.Module的父类\n",
    "    def __init__(self, C_att=False):                    # 初始化网络\n",
    "        super(LeNet, self).__init__()      #super()继承父类的构造函数，多继承需用到super函数\n",
    "        self.C_att = C_att\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, bias=False)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        if self.C_att: self.C_att1 = ChannelAttention(12, 16)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, bias=False)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32*5*5, 120, bias=False)\n",
    "        self.fc2 = nn.Linear(120, 84, bias=False)\n",
    "        self.fc3 = nn.Linear(84, 10, bias=False)\n",
    "                                     #图像参数变化\n",
    "    def forward(self, x):            # input(3, 32, 32)        \n",
    "        x = F.relu(self.conv1(x))    # output(16, 28, 28)\n",
    "        x = self.pool1(x)            # output(16, 14, 14)\n",
    "        if self.C_att: x = self.C_att1(x) # = ChannelAttention(12, 16)\n",
    "        x = F.relu(self.conv2(x))    # output(32, 10, 10)\n",
    "        x = self.pool2(x)            # output(32, 5, 5)\n",
    "        x = x.view(-1, 32*5*5)       # output(32*5*5)\n",
    "        x = F.relu(self.fc1(x))      # output(120)\n",
    "        x = F.relu(self.fc2(x))      # output(84)\n",
    "        x = self.fc3(x)              # output(10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LeNet(nn.Module):                    #继承来着nn.Module的父类\n",
    "#     def __init__(self, C_att=False):                    # 初始化网络\n",
    "#         super(LeNet, self).__init__()      #super()继承父类的构造函数，多继承需用到super函数\n",
    "#         self.C_att = C_att\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#         if self.C_att: self.C_att1 = ChannelAttention(12, 16)\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
    "#         self.fc1 = nn.Linear(32*5*5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "#                                      #图像参数变化\n",
    "#     def forward(self, x):            # input(3, 32, 32)        \n",
    "#         x = F.relu(self.conv1(x))    # output(16, 28, 28)\n",
    "#         x = self.pool1(x)            # output(16, 14, 14)\n",
    "#         if self.C_att: x = self.C_att1(x) # = ChannelAttention(12, 16)\n",
    "#         x = F.relu(self.conv2(x))    # output(32, 10, 10)\n",
    "#         x = self.pool2(x)            # output(32, 5, 5)\n",
    "#         x = x.view(-1, 32*5*5)       # output(32*5*5)\n",
    "#         x = F.relu(self.fc1(x))      # output(120)\n",
    "#         x = F.relu(self.fc2(x))      # output(84)\n",
    "#         x = self.fc3(x)              # output(10)\n",
    "#         return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dee909b94f9b71f1735c26369abf02849623765c9fac88e4d4cba156fc12504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
