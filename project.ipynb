{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import torch \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torchkeras\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dataset.ipynb\n",
    "%run MLP.ipynb\n",
    "%run CNN.ipynb\n",
    "%run RNN.ipynb\n",
    "%run train.ipynb\n",
    "%run test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = MNIST(\"train\", transform=lambda x: x / 255)\n",
    "train_num = int(len(full_dataset) * 0.9)\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_num, len(full_dataset) - train_num])  \n",
    "test_dataset = MNIST(\"test\", transform=lambda x: x / 255) \n",
    "\n",
    "# show_dataset(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1正则：用于减小复杂度\n",
    "\t- 正则项权重越大，则模型泛化性越强\n",
    "- L2正则： 同上，但L1正则化  \n",
    "\n",
    "- 随机初始化： 如正态分布、均匀分布.... 随着层数增大，容易导致输出变为0\n",
    "\t- Xavier初始化 ：适用于Tanh和Sigmoid \n",
    "\t- He初始化 ： 适用于ReLU\n",
    "\n",
    "\n",
    "多层RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-3\n",
    "# epoch = 10\n",
    "# batch_size = 256\n",
    "\n",
    "# model = LeNet()\n",
    "# model.optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "# model.loss_func = torch.nn.CrossEntropyLoss()\n",
    "# model.metric_func = lambda y_p, y: np.sum(torch.argmax(y_p.detach(), axis=1).numpy() == y.detach().numpy()) / len(y.detach().numpy())\n",
    "# model.metric_name = \"acc\"\n",
    "\n",
    "# train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "# train_model(model, epoch, train_dl, valid_dl, 640000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "\n",
      "EPOCH = 1, loss = 1.651,acc = 0.385, val_loss = 1.309, val_acc = 0.477 \n",
      "\n",
      "EPOCH = 2, loss = 1.118,acc = 0.606, val_loss = 0.961, val_acc = 0.661 \n",
      "\n",
      "EPOCH = 3, loss = 0.916,acc = 0.679, val_loss = 0.817, val_acc = 0.709 \n",
      "\n",
      "EPOCH = 4, loss = 0.764,acc = 0.742, val_loss = 0.658, val_acc = 0.766 \n",
      "\n",
      "EPOCH = 5, loss = 0.645,acc = 0.789, val_loss = 0.565, val_acc = 0.807 \n",
      "\n",
      "EPOCH = 6, loss = 0.572,acc = 0.816, val_loss = 0.507, val_acc = 0.821 \n",
      "\n",
      "EPOCH = 7, loss = 0.512,acc = 0.839, val_loss = 0.480, val_acc = 0.837 \n",
      "\n",
      "EPOCH = 8, loss = 0.472,acc = 0.853, val_loss = 0.436, val_acc = 0.848 \n",
      "\n",
      "EPOCH = 9, loss = 0.441,acc = 0.864, val_loss = 0.414, val_acc = 0.855 \n",
      "\n",
      "EPOCH = 10, loss = 0.424,acc = 0.871, val_loss = 0.412, val_acc = 0.864 \n",
      "\n",
      "Training Finished!\n",
      "\n",
      "RNN Training cost time 346.65 s\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   epoch      loss       acc  val_loss   val_acc\n",
       " 9   10.0  0.424008  0.871005   0.41169  0.863584,\n",
       " [0.006452237109343211,\n",
       "  0.004370229782881561,\n",
       "  0.003580228089182465,\n",
       "  0.0029862859812047745,\n",
       "  0.0025202985571490394,\n",
       "  0.002233873336955353,\n",
       "  0.001999384750370626,\n",
       "  0.0018443149598660292,\n",
       "  0.001724331463376681,\n",
       "  0.0016567726720262458],\n",
       " [0.005234054366747538,\n",
       "  0.0038448880712191264,\n",
       "  0.003269317736228307,\n",
       "  0.002631992588440577,\n",
       "  0.00225850780804952,\n",
       "  0.0020294424494107562,\n",
       "  0.001918202872077624,\n",
       "  0.001745773270726204,\n",
       "  0.0016548484265804292,\n",
       "  0.0016467605431874594])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "epoch = 10\n",
    "batch_size = 256\n",
    "\n",
    "# model = MLP(784, 10, True, 1, 128, \"R\")\n",
    "model = RNN(28, 10, 28, 150, \"R\")\n",
    "# model = nn_RNN(28, 10, 28, 150, \"T\")\n",
    "# model = o_RNN()\n",
    "\n",
    "# model = RNNModelScratch(28, 150, d2l.try_gpu(), get_params,\n",
    "                    #   init_rnn_state, rnn)\n",
    "\n",
    "# model = nn.RNN(input_size=28, hidden_size=128, num_layers=28)\n",
    "# inp = torch.randn(5, 28, 784)\n",
    "# h0 = torch.randn(2, 3, 20)\n",
    "# output, hn = model(inp, h0)\n",
    "\n",
    "# for i in model.parameters(): \n",
    "#     for j in i: \n",
    "#         print(j.shape)\n",
    "\n",
    "model.optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "model.loss_func = torch.nn.CrossEntropyLoss()\n",
    "model.metric_func = lambda y_p, y: np.sum(torch.argmax(y_p.detach(), axis=1).numpy() == y.detach().numpy()) / len(y.detach().numpy())\n",
    "model.metric_name = \"acc\"\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "train_model(model, epoch, train_dl, valid_dl, 400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torchw\n",
    "# for i in model.W_hh[2].parameters(): \n",
    "#     print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_model(model, test_dl)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import math\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "# batch_size, num_steps = 32, 35\n",
    "# train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "\n",
    "\n",
    "# F.one_hot(torch.tensor([0, 2]), len(vocab))\n",
    "# X = torch.arange(10).reshape((2, 5))\n",
    "# F.one_hot(X.T, 28).shape\n",
    "\n",
    "# def get_params(vocab_size, num_hiddens, device):\n",
    "#     num_inputs = num_outputs = vocab_size\n",
    "\n",
    "#     def normal(shape):\n",
    "#         return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "#     W_xh = normal((num_inputs, num_hiddens))\n",
    "#     W_hh = normal((num_hiddens, num_hiddens))\n",
    "#     b_h = torch.zeros(num_hiddens, device=device)\n",
    "#     W_hq = normal((num_hiddens, num_outputs))\n",
    "#     b_q = torch.zeros(num_outputs, device=device)\n",
    "#     params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "#     for param in params:\n",
    "#         param.requires_grad_(True)\n",
    "#     return params\n",
    "\n",
    "# def init_rnn_state(batch_size, num_hiddens, device):\n",
    "#     return (torch.zeros((batch_size, num_hiddens), device=device),)\n",
    "\n",
    "# def rnn(inputs, state, params):\n",
    "#     W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "#     H, = state\n",
    "#     outputs = []\n",
    "#     for X in inputs:\n",
    "#         H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "#         Y = torch.mm(H, W_hq) + b_q\n",
    "#         outputs.append(Y)\n",
    "#     return torch.cat(outputs, dim=0), (H,)\n",
    "\n",
    "# class RNNModelScratch:  \n",
    "#     \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "#     def __init__(self, vocab_size, num_hiddens, device, get_params,\n",
    "#                  init_state, forward_fn):\n",
    "#         self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "#         self.params = get_params(vocab_size, num_hiddens, device)\n",
    "#         self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "#     def __call__(self, X, state):\n",
    "#         X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "#         return self.forward_fn(X, state, self.params)\n",
    "\n",
    "#     def begin_state(self, batch_size, device):\n",
    "#         return self.init_state(batch_size, self.num_hiddens, device)\n",
    "\n",
    "# num_hiddens = 512\n",
    "# net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
    "#                       init_rnn_state, rnn)\n",
    "# state = net.begin_state(X.shape[0], d2l.try_gpu())\n",
    "# Y, new_state = net(X.to(d2l.try_gpu()), state)\n",
    "# Y.shape, len(new_state), new_state[0].shape\n",
    "\n",
    "# def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "#     \"\"\"训练模型一个迭代周期（定义见第8章）。\"\"\"\n",
    "#     state, timer = None, d2l.Timer()\n",
    "#     metric = d2l.Accumulator(2)\n",
    "#     for X, Y in train_iter:\n",
    "#         if state is None or use_random_iter:\n",
    "#             state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "#         else:\n",
    "#             if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "#                 state.detach_()\n",
    "#             else:\n",
    "#                 for s in state:\n",
    "#                     s.detach_()\n",
    "#         y = Y.T.reshape(-1)\n",
    "#         X, y = X.to(device), y.to(device)\n",
    "#         y_hat, state = net(X, state)\n",
    "#         l = loss(y_hat, y.long()).mean()\n",
    "#         if isinstance(updater, torch.optim.Optimizer):\n",
    "#             updater.zero_grad()\n",
    "#             l.backward()\n",
    "#             grad_clipping(net, 1)\n",
    "#             updater.step()\n",
    "#         else:\n",
    "#             l.backward()\n",
    "#             grad_clipping(net, 1)\n",
    "#             updater(batch_size=1)\n",
    "#         metric.add(l * y.numel(), y.numel())\n",
    "#     return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n",
    "\n",
    "# def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "#               use_random_iter=False):\n",
    "#     \"\"\"训练模型（定义见第8章）。\"\"\"\n",
    "#     loss = nn.CrossEntropyLoss()\n",
    "#     animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "#                             legend=['train'], xlim=[10, num_epochs])\n",
    "#     if isinstance(net, nn.Module):\n",
    "#         updater = torch.optim.SGD(net.parameters(), lr)\n",
    "#     else:\n",
    "#         updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "#     predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device,\n",
    "#                                      use_random_iter)\n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             print(predict('time traveller'))\n",
    "#             animator.add(epoch + 1, [ppl])\n",
    "#     print(f'困惑度 {ppl:.1f}, {speed:.1f} 标记/秒 {str(device)}')\n",
    "#     print(predict('time traveller'))\n",
    "#     print(predict('traveller'))\n",
    "\n",
    "# train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dee909b94f9b71f1735c26369abf02849623765c9fac88e4d4cba156fc12504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
